{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report\n",
    "* Create a **300-600 word written report** called \"wrangle_report.pdf\" or \"wrangle_report.html\" that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the process of Data Wrangling of tweet data from the Twitter user  WeRateDogs, I have found several issues in the Quality and Tidiness of data collected by different means.\n",
    "\n",
    "I have analyzed, cleaned and combined all the data into a new DataFrame and stored it in twitter_archive_master.csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project aims to gather data provided, to create analysis about the tweets and the predicted dogâ€™s breed.\n",
    "\n",
    "The data wrangling procedure is as follows:\n",
    "1. Gathering data\n",
    "2. Assessing data\n",
    "3. Cleaning data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Gathering Data\n",
    "I have gathered the files twitter_archive_enhanced.csv and image_predictions.tsv, which are provided by Udacity in the lesson: Data Wrangling\n",
    "\n",
    "The twitter_archive_enhanced.csv file contains basic tweet data (tweet ID, timestamp, text, etc.) for 2356 of their tweets as they stood on August 1, 2017. As I needed further information from the WeRateDogs user, I gathered data  the text_json.txt for the above mentioned period (querying by tweet_id present in twitter_archive_enhanced.csv)\n",
    "\n",
    "The gathered data are loaded into three different DataFrame,namely:\n",
    " df1 = twitter_archive_enhanced.csv\n",
    " df2 = image_predictions.tsv\n",
    " df = tweet.json\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Assessing Data\n",
    "The two types of Data Assessment performed,\n",
    "\n",
    "Visual assessment: Each dataset is displayed in the Jupyter Notebook for visual assessment. I also used Excel worksheets.\n",
    "Programmatic assessment: Used the pandas function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Data Cleaning\n",
    "I made acopy of each piece of data using .copy method. The reason is so that i could still view the original drty and messy datasets.\n",
    "They are:\n",
    "\n",
    "df1_clean = df1.copy()\n",
    "df2_clean = df2.copy()\n",
    "df_clean = df.copy()\n",
    "\n",
    "Further steps i managed to accomplish is as follows:\n",
    "Quality issues\n",
    "1.Underscore '' present instead of space in dog breeds (p1,p2,p3) and Few names with '-' present retweetfavorite_count table\n",
    "\n",
    "2.There are unintrested columns : retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp, in_reply_to_status_id, in_reply_to_user_id\n",
    "\n",
    "3.Erroneous datatypes : tweet_id and timestamp\n",
    "\n",
    "4.Consolidation of dog style column into one\n",
    "\n",
    "5.Droping not needed columns i.e. expanded_urls in df1 and img_num in df2\n",
    "6.Rating denominator\n",
    "7.Replacing doubtful words \n",
    "8. Completing tag present instead of source name in source\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Conclusion,\n",
    "I have stored the wrangled data in twitter_archive_master.csv file ready for Data Analysis. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
